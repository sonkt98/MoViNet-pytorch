{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ambient-trail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "sys.path.append('..')\n",
    "from movinets import MoViNet\n",
    "from movinets.config import _C\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-private",
   "metadata": {},
   "source": [
    "# Example of weight loading\n",
    "## Loading the weights in Tensorflow\n",
    "The link used is the one for the A5 model, and this example will continue loading the weights for that specific model. <br>\n",
    "To load the weights of a different model change now the link of the model. <br>\n",
    "A0 : \"https://tfhub.dev/tensorflow/movinet/a0/base/kinetics-600/classification/2\"<br>\n",
    "A1 : \"https://tfhub.dev/tensorflow/movinet/a1/base/kinetics-600/classification/2\"<br>\n",
    "A2 : \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/2\"<br>\n",
    "A3 : \"https://tfhub.dev/tensorflow/movinet/a3/base/kinetics-600/classification/2\"<br>\n",
    "A4 : \"https://tfhub.dev/tensorflow/movinet/a4/base/kinetics-600/classification/2\"<br>\n",
    "A5 : \"https://tfhub.dev/tensorflow/movinet/a5/base/kinetics-600/classification/2\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specific-tampa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value for attr 'data_format' of \"NDHWC\" is not in the list of allowed values: \"NHWC\", \"NCHW\"\n\t; NodeDef: {{node FusedBatchNormV3}}; Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=is_training:bool,default=true>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    500\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 501\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'data_format' of \"NDHWC\" is not in the list of allowed values: \"NHWC\", \"NCHW\"\n\t; NodeDef: {{node FusedBatchNormV3}}; Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=is_training:bool,default=true>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9f1cfa3cdc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder = hub.KerasLayer(\n\u001b[0;32m----> 6\u001b[0;31m     \"https://tfhub.dev/tensorflow/movinet/a0/base/kinetics-600/classification/3\")\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Important: due to a bug in the tf.nn.conv3d CPU implementation, we must\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m   \"\"\"\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    541\u001b[0m       loader = loader_cls(object_graph_proto,\n\u001b[1;32m    542\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                           export_dir)\n\u001b[0m\u001b[1;32m    544\u001b[0m       \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m     self._concrete_functions = (\n\u001b[1;32m    113\u001b[0m         function_deserialization.load_function_def_library(\n\u001b[0;32m--> 114\u001b[0;31m             meta_graph.graph_def.library))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# import).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     func_graph = function_def_lib.function_def_to_graph(\n\u001b[0;32m--> 312\u001b[0;31m         copy, copy_functions=False)\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_list_function_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_function_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, input_shapes, copy_functions)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Add all function nodes to the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def_for_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def_for_function\u001b[0;34m(graph_def, name)\u001b[0m\n\u001b[1;32m    410\u001b[0m   \u001b[0;34m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m   return _import_graph_def_internal(\n\u001b[0;32m--> 412\u001b[0;31m       graph_def, validate_colocation_constraints=False, name=name)\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value for attr 'data_format' of \"NDHWC\" is not in the list of allowed values: \"NHWC\", \"NCHW\"\n\t; NodeDef: {{node FusedBatchNormV3}}; Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=is_training:bool,default=true>"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(\n",
    "    shape=[None, None, None, 3],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/movinet/a0/base/kinetics-600/classification/3\")\n",
    "\n",
    "# Important: due to a bug in the tf.nn.conv3d CPU implementation, we must\n",
    "# compile with tf.function to enforce correct behavior. Otherwise, the output\n",
    "# on CPU may be incorrect.\n",
    "encoder.call = tf.function(encoder.call, experimental_compile=True)\n",
    "\n",
    "# [batch_size, 600]\n",
    "outputs = encoder(dict(image=inputs))\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "#save the weights of the pretrained model in a list\n",
    "loaded_list = []\n",
    "for item in encoder.variables:\n",
    "  loaded_list.append((item.name,item.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-raise",
   "metadata": {},
   "source": [
    "## Loading Weights in the pytorch model\n",
    "Change now the model name with the one that matches the TF model loaded in the previous step.\n",
    "Different models have sligtly different behaviour.\n",
    "```python\n",
    "model_name = \"modelA0\" \n",
    "model_name = \"modelA1\"\n",
    "model_name = \"modelA2\"\n",
    "model_name = \"modelA3\"  \n",
    "model_name = \"modelA4\"   \n",
    "model_name = \"modelA5\" \n",
    "```\n",
    "Change the loaded pytorch model <br>\n",
    "```python\n",
    "model = MoViNet(_C.MODEL.MoViNetA0, 600,causal = False, tf_like = True)\n",
    "model = MoViNet(_C.MODEL.MoViNetA1, 600,causal = False, tf_like = True)\n",
    "model = MoViNet(_C.MODEL.MoViNetA2, 600,causal = False, tf_like = True)\n",
    "model = MoViNet(_C.MODEL.MoViNetA3, 600,causal = False, tf_like = True)\n",
    "model = MoViNet(_C.MODEL.MoViNetA4, 600,causal = False, tf_like = True)\n",
    "model = MoViNet(_C.MODEL.MoViNetA5, 600,causal = False, tf_like = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "addressed-inspiration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0c59a758925b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#creating the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey_translate_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mweight_translate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoViNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMoViNetA0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_list' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"modelA0\" \n",
    "\n",
    "def key_translate_base( k):\n",
    "    k = (k\n",
    "    .replace(\"classifier_head/head/conv3d/\" ,\"classifier.0.conv_1.conv3d.\")\n",
    "    .replace(\"classifier_head/classifier/conv3d/\", \"classifier.3.conv_1.conv3d.\")\n",
    "    .replace(\"se/se_reduce/conv3d/\",\"se.fc1.conv_1.conv3d.\")\n",
    "    .replace(\"se/se_expand/conv3d/\",\"se.fc2.conv_1.conv3d.\")\n",
    "    .replace(\"stem/stem/\", \"conv1.conv_1.\")\n",
    "    .replace(\"conv3d/\", \"conv3d.\")\n",
    "    .replace(\"kernel:0\",\"weight\")\n",
    "    .replace(\"bias:0\",\"bias\")\n",
    "    .replace(\"bn/gamma:0\",\"norm.weight\")\n",
    "    .replace(\"bn/beta:0\",\"norm.bias\")\n",
    "    .replace(\"bn/moving_mean:0\",\"norm.running_mean\")\n",
    "    .replace(\"bn/moving_variance:0\",\"norm.running_var\")\n",
    "    .replace(\"skip/skip_project/\",\"res.1.conv_1.\")\n",
    "    .replace(\"expansion/\",\"expand.conv_1.\")\n",
    "    .replace(\"feature/\",\"deep.conv_1.\")\n",
    "    .replace(\"projection/\",\"project.conv_1.\")\n",
    "    .replace(\"scale:0\", \"alpha\")\n",
    "    .replace(\"head/project/\", \"conv7.conv_1.\"))\n",
    "    for i in range(5):\n",
    "        for j in range(20):\n",
    "            k=k.replace(f\"b{i}/l{j}/bneck/\", f\"blocks.b{i}_l{j}.\").replace(f\"b{i}/l{j}/\", f\"blocks.b{i}_l{j}.\")\n",
    "    if (model_name == \"modelA3\" or model_name == \"modelA5\") and \"b3_l0\" in k:\n",
    "        k = k.replace(\"res.1.\",\"res.0.\")\n",
    "    return k\n",
    "\n",
    "def key_translate_stream( k):\n",
    "    k = (k.replace(\"feature/conv2d/depthwise_conv2d/depthwise_kernel:0\",\"deep.conv_1.conv2d.weight\")\n",
    "         .replace(\"feature/conv2d_temporal/depthwise_conv2d_1/depthwise_kernel:0\",\"deep.conv_2.conv2d.weight\")\n",
    "         \n",
    "         .replace(\"feature/bn/\",\"deep.conv_1.norm.\")\n",
    "         .replace(\"feature/bn_temporal/\",\"deep.conv_2.norm.\")\n",
    "         .replace(\"expansion/conv2d/conv2d/\",\"expand.conv_1.conv2d.\")\n",
    "         .replace(\"expansion/bn/\",\"expand.conv_1.norm.\")\n",
    "         .replace(\"projection/conv2d/conv2d_3/\",\"project.conv_1.conv2d.\")\n",
    "         .replace(\"projection/bn/\",\"project.conv_1.norm.\")\n",
    "         .replace(\"se/se_reduce/conv2d/conv2d_1/\",\"se.fc1.conv_1.conv2d.\")\n",
    "         .replace(\"se/se_expand/conv2d/conv2d_2/\",\"se.fc2.conv_1.conv2d.\")\n",
    "         .replace(\"skip/skip_project/conv2d/conv2d_4/\", \"res.1.conv_1.conv2d.\")\n",
    "    .replace(\"skip/skip_project/bn/\" ,\"res.1.conv_1.norm.\")\n",
    "         .replace(\"classifier_head/head/conv2d/conv2d/\" ,\"classifier.0.conv_1.conv2d.\")\n",
    "    \n",
    "         .replace(\"classifier_head/classifier/conv2d/conv2d_1/\" ,\"classifier.3.conv_1.conv2d.\")\n",
    "         .replace(\"head/project/bn/\",\"conv7.conv_1.norm.\")\n",
    "         .replace(\"head/project/conv2d/conv2d/\" ,\"conv7.conv_1.conv2d.\")\n",
    "         .replace(\"stem/stem/bn/\",\"conv1.conv_1.norm.\")\n",
    "         .replace(\"stem/stem/conv2d/conv2d/\" ,\"conv1.conv_1.conv2d.\")\n",
    "    .replace(\"kernel:0\",\"weight\")\n",
    "    .replace(\"bias:0\",\"bias\")\n",
    "    .replace(\"gamma:0\",\"weight\")\n",
    "    .replace(\"beta:0\",\"bias\")\n",
    "    .replace(\"moving_mean:0\",\"running_mean\")\n",
    "    .replace(\"moving_variance:0\",\"running_var\")\n",
    "    .replace(\"scale:0\", \"alpha\")\n",
    "        )\n",
    "    for i in range(5):\n",
    "        for j in range(20):\n",
    "            k=k.replace(f\"b{i}/l{j}/bneck/\", f\"blocks.b{i}_l{j}.\").replace(f\"b{i}/l{j}/\", f\"blocks.b{i}_l{j}.\")\n",
    "    if (model_name == \"modelA3\" or model_name == \"modelA5\") and \"b3_l0\" in k:\n",
    "        k = k.replace(\"res.1.\",\"res.0.\")\n",
    "    return k\n",
    "\n",
    "def weight_translate( w):\n",
    "    if len(w.shape)==5:\n",
    "        w = rearrange(w, \"d h w c_in c_out -> c_out c_in d h w\")\n",
    "    if len(w.shape)==4:\n",
    "        #w = rearrange(w, \"h w c_in c_out -> c_out c_in h w\")\n",
    "        if \"feature\" in name:\n",
    "            w = rearrange(w, \"h w c_out c_in-> c_out c_in h w\")\n",
    "        else:\n",
    "            w = rearrange(w, \"h w c_in c_out -> c_out c_in h w\")\n",
    "    return torch.tensor(w)\n",
    "\n",
    "#creating the dictionary\n",
    "param_dict = {key_translate_base(name ) : weight_translate( item) for i,(name,item) in enumerate(loaded_list)}\n",
    "\n",
    "model = MoViNet(_C.MODEL.MoViNetA0, causal = False, num_classes = 600, tf_like = True)\n",
    "#load the dictionary\n",
    "model.load_state_dict(param_dict)\n",
    "#save the model\n",
    "torch.save(model.state_dict(), \"./modelA0_statedict_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sustained-agriculture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(554)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from six.moves import urllib\n",
    "from PIL import Image\n",
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/8/84/Ski_Famille_-_Family_Ski_Holidays.jpg'\n",
    "image_height = 172\n",
    "image_width = 172\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with urllib.request.urlopen(image_url) as f:\n",
    "  image = Image.open(f).resize((image_height, image_width))\n",
    "video = tf.reshape(np.array(image), [1, 1, image_height, image_width, 3])\n",
    "video = tf.cast(video, tf.float32) / 255.\n",
    "video_2 = rearrange(torch.from_numpy(video.numpy()), \"b t h w c-> b c t h w\")\n",
    "model.eval()\n",
    "model.clean_activation_buffers()\n",
    "result = model(video_2)\n",
    "torch.argmax(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-argentina",
   "metadata": {},
   "source": [
    "Run the model and output the predicted label. Expected output should be skiing (labels 464-467). E.g., 465 = \"skiing crosscountry\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-destiny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
